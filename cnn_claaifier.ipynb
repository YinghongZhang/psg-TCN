{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plot\n",
    "from data.ml_dataset import BODataSet\n",
    "from data.cnn_dataset import BloodOxyen\n",
    "from models.tcn import TCN\n",
    "from models.google_net import GoogleNet\n",
    "from models.nasnet import NASNET\n",
    "from models.vgg16 import vgg16\n",
    "from models.dense import DenseNet\n",
    "from models.resnet import ResNet\n",
    "from models.psgnet import PSGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络参数\n",
    "root = './data/trimer/pg'\n",
    "batch_size = 4\n",
    "input_channels = 1\n",
    "n_classes = 3\n",
    "seq_length = int(90 / input_channels)\n",
    "epochs = 50\n",
    "steps = 0\n",
    "levels = 8\n",
    "nhid = 256\n",
    "dropout = 0.05\n",
    "channel_sizes = [nhid] * levels\n",
    "kernel_size = 3\n",
    "lr = 0.001\n",
    "optim = \"SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算F1 score 准确率 召回率\n",
    "def score(Y, pre_Y):\n",
    "    print(\"Precision_score: %.1f \" % (metrics.precision_score(Y, pre_Y, average='weighted')*100))\n",
    "    print(\"recall_score: %.1f \" % (metrics.recall_score(Y, pre_Y, average='weighted')*100))\n",
    "    print(\"f1_score: %.1f \" % (metrics.f1_score(Y, pre_Y, average='weighted')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "何文东\n",
      "杨梓恒\n",
      "林颖晞\n",
      "舒辰熙\n",
      "陈知浅\n",
      "高子羲\n",
      "何文东\n",
      "杨梓恒\n",
      "林颖晞\n",
      "舒辰熙\n",
      "陈知浅\n",
      "高子羲\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_set = BloodOxyenForTest(root=root, train=True, extend=False)\n",
    "val_set = BloodOxyenForTest(root=root, train=False, extend=False)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3360.0, 985.0, 1412.0], 4029)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.class_total, len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\F\\PSG-AI\\models\\google_net.py:113: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(m.weight)\n",
      "C:\\F\\PSG-AI\\models\\dense.py:91: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "# 初始化网路，verbose 参数决定运行时是否print网络规模（输入输出层数）\n",
    "verbose = False\n",
    "gn_model = GoogleNet(input_channels, n_classes, verbose=verbose)\n",
    "na_model = NASNET(input_channels, n_classes, verbose=verbose)\n",
    "vgg_model = vgg16(num_classes=3, verbose=verbose)\n",
    "ds_model = dense.densenet121(verbose=verbose)\n",
    "re_model = resnet.ResNet(3, verbose=verbose)\n",
    "tcn_model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n",
    "simple_model = simpleLinear(1, 3, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整学习率函数\n",
    "def adjust_learning_rate(lr, optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_score: 59.4 \n",
      "recall_score: 61.6 \n",
      "f1_score: 59.7 \n",
      "Epoch 0. Train Loss: 0.22236728223656033 \n",
      " Accuracy of network on 1728 events: 61.6%\n",
      "Accuracy of network on 681 abevents: 29.5%\n",
      "Accuracy on 1047 normal events: 82.4%\n",
      "Accuracy on 234 hypopnea events: 18.4%\n",
      "Accuracy on 447 apnea events: 35.3% \n",
      "\n",
      "Precision_score: 62.7 \n",
      "recall_score: 55.5 \n",
      "f1_score: 57.8 \n",
      "Epoch 1. Train Loss: 0.2016617325591106 \n",
      " Accuracy of network on 1728 events: 55.5%\n",
      "Accuracy of network on 681 abevents: 37.4%\n",
      "Accuracy on 1047 normal events: 67.2%\n",
      "Accuracy on 234 hypopnea events: 40.2%\n",
      "Accuracy on 447 apnea events: 36.0% \n",
      "\n",
      "Precision_score: 59.1 \n",
      "recall_score: 52.7 \n",
      "f1_score: 54.1 \n",
      "Epoch 2. Train Loss: 0.1979343357490647 \n",
      " Accuracy of network on 1728 events: 52.7%\n",
      "Accuracy of network on 681 abevents: 30.8%\n",
      "Accuracy on 1047 normal events: 66.9%\n",
      "Accuracy on 234 hypopnea events: 41.0%\n",
      "Accuracy on 447 apnea events: 25.5% \n",
      "\n",
      "Precision_score: 64.9 \n",
      "recall_score: 67.7 \n",
      "f1_score: 61.6 \n",
      "Epoch 3. Train Loss: 0.19371756850992605 \n",
      " Accuracy of network on 1728 events: 67.7%\n",
      "Accuracy of network on 681 abevents: 24.7%\n",
      "Accuracy on 1047 normal events: 95.6%\n",
      "Accuracy on 234 hypopnea events: 7.3%\n",
      "Accuracy on 447 apnea events: 33.8% \n",
      "\n",
      "Precision_score: 63.0 \n",
      "recall_score: 61.8 \n",
      "f1_score: 60.8 \n",
      "Epoch 4. Train Loss: 0.19073137785193642 \n",
      " Accuracy of network on 1728 events: 61.8%\n",
      "Accuracy of network on 681 abevents: 31.3%\n",
      "Accuracy on 1047 normal events: 81.7%\n",
      "Accuracy on 234 hypopnea events: 29.1%\n",
      "Accuracy on 447 apnea events: 32.4% \n",
      "\n",
      "Precision_score: 60.4 \n",
      "recall_score: 64.1 \n",
      "f1_score: 61.9 \n",
      "Epoch 5. Train Loss: 0.18514097645068825 \n",
      " Accuracy of network on 1728 events: 64.1%\n",
      "Accuracy of network on 681 abevents: 37.0%\n",
      "Accuracy on 1047 normal events: 81.8%\n",
      "Accuracy on 234 hypopnea events: 9.0%\n",
      "Accuracy on 447 apnea events: 51.7% \n",
      "\n",
      "Precision_score: 62.1 \n",
      "recall_score: 67.0 \n",
      "f1_score: 63.1 \n",
      "Epoch 6. Train Loss: 0.18724506961024112 \n",
      " Accuracy of network on 1728 events: 67.0%\n",
      "Accuracy of network on 681 abevents: 35.7%\n",
      "Accuracy on 1047 normal events: 87.3%\n",
      "Accuracy on 234 hypopnea events: 5.6%\n",
      "Accuracy on 447 apnea events: 51.5% \n",
      "\n",
      "Precision_score: 58.8 \n",
      "recall_score: 63.4 \n",
      "f1_score: 53.9 \n",
      "Epoch 7. Train Loss: 0.18808961608630903 \n",
      " Accuracy of network on 1728 events: 63.4%\n",
      "Accuracy of network on 681 abevents: 10.3%\n",
      "Accuracy on 1047 normal events: 97.9%\n",
      "Accuracy on 234 hypopnea events: 6.8%\n",
      "Accuracy on 447 apnea events: 12.1% \n",
      "\n",
      "Precision_score: 65.0 \n",
      "recall_score: 66.8 \n",
      "f1_score: 64.1 \n",
      "Epoch 8. Train Loss: 0.18464860851562298 \n",
      " Accuracy of network on 1728 events: 66.8%\n",
      "Accuracy of network on 681 abevents: 31.0%\n",
      "Accuracy on 1047 normal events: 90.1%\n",
      "Accuracy on 234 hypopnea events: 26.5%\n",
      "Accuracy on 447 apnea events: 33.3% \n",
      "\n",
      "Precision_score: 61.3 \n",
      "recall_score: 65.9 \n",
      "f1_score: 59.7 \n",
      "Epoch 9. Train Loss: 0.18313235361068944 \n",
      " Accuracy of network on 1728 events: 65.9%\n",
      "Accuracy of network on 681 abevents: 20.9%\n",
      "Accuracy on 1047 normal events: 95.1%\n",
      "Accuracy on 234 hypopnea events: 8.5%\n",
      "Accuracy on 447 apnea events: 27.3% \n",
      "\n",
      "Precision_score: 61.5 \n",
      "recall_score: 66.8 \n",
      "f1_score: 62.6 \n",
      "Epoch 10. Train Loss: 0.18333376548328126 \n",
      " Accuracy of network on 1728 events: 66.8%\n",
      "Accuracy of network on 681 abevents: 30.1%\n",
      "Accuracy on 1047 normal events: 90.7%\n",
      "Accuracy on 234 hypopnea events: 8.1%\n",
      "Accuracy on 447 apnea events: 41.6% \n",
      "\n",
      "Precision_score: 63.9 \n",
      "recall_score: 68.3 \n",
      "f1_score: 64.1 \n",
      "Epoch 11. Train Loss: 0.18223227321043828 \n",
      " Accuracy of network on 1728 events: 68.3%\n",
      "Accuracy of network on 681 abevents: 31.3%\n",
      "Accuracy on 1047 normal events: 92.4%\n",
      "Accuracy on 234 hypopnea events: 11.1%\n",
      "Accuracy on 447 apnea events: 41.8% \n",
      "\n",
      "Precision_score: 66.0 \n",
      "recall_score: 66.5 \n",
      "f1_score: 65.2 \n",
      "Epoch 12. Train Loss: 0.18149539168862144 \n",
      " Accuracy of network on 1728 events: 66.5%\n",
      "Accuracy of network on 681 abevents: 35.7%\n",
      "Accuracy on 1047 normal events: 86.5%\n",
      "Accuracy on 234 hypopnea events: 32.5%\n",
      "Accuracy on 447 apnea events: 37.4% \n",
      "\n",
      "Precision_score: 62.1 \n",
      "recall_score: 66.0 \n",
      "f1_score: 60.2 \n",
      "Epoch 13. Train Loss: 0.182728584666248 \n",
      " Accuracy of network on 1728 events: 66.0%\n",
      "Accuracy of network on 681 abevents: 21.1%\n",
      "Accuracy on 1047 normal events: 95.2%\n",
      "Accuracy on 234 hypopnea events: 11.5%\n",
      "Accuracy on 447 apnea events: 26.2% \n",
      "\n",
      "Precision_score: 63.6 \n",
      "recall_score: 68.5 \n",
      "f1_score: 63.5 \n",
      "Epoch 14. Train Loss: 0.1777893092236409 \n",
      " Accuracy of network on 1728 events: 68.5%\n",
      "Accuracy of network on 681 abevents: 31.7%\n",
      "Accuracy on 1047 normal events: 92.4%\n",
      "Accuracy on 234 hypopnea events: 5.6%\n",
      "Accuracy on 447 apnea events: 45.4% \n",
      "\n",
      "Precision_score: 61.0 \n",
      "recall_score: 66.7 \n",
      "f1_score: 62.8 \n",
      "Epoch 15. Train Loss: 0.179336114686013 \n",
      " Accuracy of network on 1728 events: 66.7%\n",
      "Accuracy of network on 681 abevents: 38.5%\n",
      "Accuracy on 1047 normal events: 85.1%\n",
      "Accuracy on 234 hypopnea events: 2.1%\n",
      "Accuracy on 447 apnea events: 57.5% \n",
      "\n",
      "Precision_score: 63.9 \n",
      "recall_score: 67.2 \n",
      "f1_score: 64.4 \n",
      "Epoch 16. Train Loss: 0.18115348918661642 \n",
      " Accuracy of network on 1728 events: 67.2%\n",
      "Accuracy of network on 681 abevents: 34.9%\n",
      "Accuracy on 1047 normal events: 88.3%\n",
      "Accuracy on 234 hypopnea events: 17.9%\n",
      "Accuracy on 447 apnea events: 43.8% \n",
      "\n",
      "Precision_score: 63.5 \n",
      "recall_score: 67.2 \n",
      "f1_score: 61.6 \n",
      "Epoch 17. Train Loss: 0.17920885652999008 \n",
      " Accuracy of network on 1728 events: 67.2%\n",
      "Accuracy of network on 681 abevents: 24.4%\n",
      "Accuracy on 1047 normal events: 95.1%\n",
      "Accuracy on 234 hypopnea events: 9.8%\n",
      "Accuracy on 447 apnea events: 32.0% \n",
      "\n",
      "Precision_score: 61.5 \n",
      "recall_score: 66.1 \n",
      "f1_score: 60.6 \n",
      "Epoch 18. Train Loss: 0.17866410601298843 \n",
      " Accuracy of network on 1728 events: 66.1%\n",
      "Accuracy of network on 681 abevents: 22.0%\n",
      "Accuracy on 1047 normal events: 94.7%\n",
      "Accuracy on 234 hypopnea events: 12.0%\n",
      "Accuracy on 447 apnea events: 27.3% \n",
      "\n",
      "Precision_score: 64.6 \n",
      "recall_score: 67.2 \n",
      "f1_score: 62.4 \n",
      "Epoch 19. Train Loss: 0.17805940994600353 \n",
      " Accuracy of network on 1728 events: 67.2%\n",
      "Accuracy of network on 681 abevents: 24.7%\n",
      "Accuracy on 1047 normal events: 94.8%\n",
      "Accuracy on 234 hypopnea events: 17.9%\n",
      "Accuracy on 447 apnea events: 28.2% \n",
      "\n",
      "Precision_score: 64.5 \n",
      "recall_score: 68.2 \n",
      "f1_score: 65.5 \n",
      "Epoch 20. Train Loss: 0.17844889272669168 \n",
      " Accuracy of network on 1728 events: 68.2%\n",
      "Accuracy of network on 681 abevents: 38.9%\n",
      "Accuracy on 1047 normal events: 87.3%\n",
      "Accuracy on 234 hypopnea events: 13.2%\n",
      "Accuracy on 447 apnea events: 52.3% \n",
      "\n",
      "Precision_score: 61.5 \n",
      "recall_score: 66.0 \n",
      "f1_score: 60.8 \n",
      "Epoch 21. Train Loss: 0.17767737515143112 \n",
      " Accuracy of network on 1728 events: 66.0%\n",
      "Accuracy of network on 681 abevents: 22.9%\n",
      "Accuracy on 1047 normal events: 94.0%\n",
      "Accuracy on 234 hypopnea events: 12.0%\n",
      "Accuracy on 447 apnea events: 28.6% \n",
      "\n",
      "Precision_score: 63.4 \n",
      "recall_score: 67.6 \n",
      "f1_score: 64.1 \n",
      "Epoch 22. Train Loss: 0.17717062532679856 \n",
      " Accuracy of network on 1728 events: 67.6%\n",
      "Accuracy of network on 681 abevents: 32.9%\n",
      "Accuracy on 1047 normal events: 90.2%\n",
      "Accuracy on 234 hypopnea events: 13.2%\n",
      "Accuracy on 447 apnea events: 43.2% \n",
      "\n",
      "Precision_score: 63.6 \n",
      "recall_score: 67.5 \n",
      "f1_score: 62.0 \n",
      "Epoch 23. Train Loss: 0.1767625762429389 \n",
      " Accuracy of network on 1728 events: 67.5%\n",
      "Accuracy of network on 681 abevents: 24.1%\n",
      "Accuracy on 1047 normal events: 95.8%\n",
      "Accuracy on 234 hypopnea events: 11.1%\n",
      "Accuracy on 447 apnea events: 30.9% \n",
      "\n",
      "Precision_score: 64.4 \n",
      "recall_score: 68.4 \n",
      "f1_score: 65.5 \n",
      "Epoch 24. Train Loss: 0.17922972891822764 \n",
      " Accuracy of network on 1728 events: 68.4%\n",
      "Accuracy of network on 681 abevents: 36.1%\n",
      "Accuracy on 1047 normal events: 89.4%\n",
      "Accuracy on 234 hypopnea events: 14.5%\n",
      "Accuracy on 447 apnea events: 47.4% \n",
      "\n",
      "Precision_score: 63.3 \n",
      "recall_score: 67.7 \n",
      "f1_score: 63.5 \n",
      "Epoch 25. Train Loss: 0.17690971503706895 \n",
      " Accuracy of network on 1728 events: 67.7%\n",
      "Accuracy of network on 681 abevents: 30.7%\n",
      "Accuracy on 1047 normal events: 91.8%\n",
      "Accuracy on 234 hypopnea events: 10.7%\n",
      "Accuracy on 447 apnea events: 41.2% \n",
      "\n",
      "Precision_score: 62.0 \n",
      "recall_score: 65.7 \n",
      "f1_score: 63.2 \n",
      "Epoch 26. Train Loss: 0.17437666089274684 \n",
      " Accuracy of network on 1728 events: 65.7%\n",
      "Accuracy of network on 681 abevents: 36.9%\n",
      "Accuracy on 1047 normal events: 84.4%\n",
      "Accuracy on 234 hypopnea events: 11.1%\n",
      "Accuracy on 447 apnea events: 50.3% \n",
      "\n",
      "Precision_score: 64.2 \n",
      "recall_score: 67.7 \n",
      "f1_score: 63.7 \n",
      "Epoch 27. Train Loss: 0.17517699017754618 \n",
      " Accuracy of network on 1728 events: 67.7%\n",
      "Accuracy of network on 681 abevents: 28.2%\n",
      "Accuracy on 1047 normal events: 93.4%\n",
      "Accuracy on 234 hypopnea events: 16.2%\n",
      "Accuracy on 447 apnea events: 34.5% \n",
      "\n",
      "Precision_score: 65.2 \n",
      "recall_score: 69.0 \n",
      "f1_score: 65.6 \n",
      "Epoch 28. Train Loss: 0.17138688561970497 \n",
      " Accuracy of network on 1728 events: 69.0%\n",
      "Accuracy of network on 681 abevents: 34.4%\n",
      "Accuracy on 1047 normal events: 91.6%\n",
      "Accuracy on 234 hypopnea events: 15.0%\n",
      "Accuracy on 447 apnea events: 44.5% \n",
      "\n",
      "Precision_score: 63.2 \n",
      "recall_score: 67.0 \n",
      "f1_score: 63.6 \n",
      "Epoch 29. Train Loss: 0.1767781014424987 \n",
      " Accuracy of network on 1728 events: 67.0%\n",
      "Accuracy of network on 681 abevents: 30.4%\n",
      "Accuracy on 1047 normal events: 90.7%\n",
      "Accuracy on 234 hypopnea events: 16.7%\n",
      "Accuracy on 447 apnea events: 37.6% \n",
      "\n",
      "Precision_score: 65.1 \n",
      "recall_score: 66.6 \n",
      "f1_score: 63.6 \n",
      "Epoch 30. Train Loss: 0.17598344420204687 \n",
      " Accuracy of network on 1728 events: 66.6%\n",
      "Accuracy of network on 681 abevents: 29.2%\n",
      "Accuracy on 1047 normal events: 90.8%\n",
      "Accuracy on 234 hypopnea events: 26.9%\n",
      "Accuracy on 447 apnea events: 30.4% \n",
      "\n",
      "Precision_score: 64.6 \n",
      "recall_score: 68.7 \n",
      "f1_score: 64.8 \n",
      "Epoch 31. Train Loss: 0.17034962894633082 \n",
      " Accuracy of network on 1728 events: 68.7%\n",
      "Accuracy of network on 681 abevents: 32.5%\n",
      "Accuracy on 1047 normal events: 92.3%\n",
      "Accuracy on 234 hypopnea events: 13.2%\n",
      "Accuracy on 447 apnea events: 42.5% \n",
      "\n",
      "Precision_score: 66.0 \n",
      "recall_score: 69.2 \n",
      "f1_score: 66.6 \n",
      "Epoch 32. Train Loss: 0.16719249240197862 \n",
      " Accuracy of network on 1728 events: 69.2%\n",
      "Accuracy of network on 681 abevents: 37.7%\n",
      "Accuracy on 1047 normal events: 89.6%\n",
      "Accuracy on 234 hypopnea events: 20.1%\n",
      "Accuracy on 447 apnea events: 47.0% \n",
      "\n",
      "Precision_score: 64.7 \n",
      "recall_score: 68.1 \n",
      "f1_score: 65.1 \n",
      "Epoch 33. Train Loss: 0.1652683064907412 \n",
      " Accuracy of network on 1728 events: 68.1%\n",
      "Accuracy of network on 681 abevents: 33.0%\n",
      "Accuracy on 1047 normal events: 90.8%\n",
      "Accuracy on 234 hypopnea events: 20.1%\n",
      "Accuracy on 447 apnea events: 39.8% \n",
      "\n",
      "Precision_score: 65.2 \n",
      "recall_score: 68.6 \n",
      "f1_score: 65.9 \n",
      "Epoch 34. Train Loss: 0.16586002599855723 \n",
      " Accuracy of network on 1728 events: 68.6%\n",
      "Accuracy of network on 681 abevents: 36.0%\n",
      "Accuracy on 1047 normal events: 89.8%\n",
      "Accuracy on 234 hypopnea events: 18.8%\n",
      "Accuracy on 447 apnea events: 45.0% \n",
      "\n",
      "Precision_score: 65.7 \n",
      "recall_score: 69.3 \n",
      "f1_score: 66.2 \n",
      "Epoch 35. Train Loss: 0.16645673308110054 \n",
      " Accuracy of network on 1728 events: 69.3%\n",
      "Accuracy of network on 681 abevents: 36.1%\n",
      "Accuracy on 1047 normal events: 90.8%\n",
      "Accuracy on 234 hypopnea events: 16.7%\n",
      "Accuracy on 447 apnea events: 46.3% \n",
      "\n",
      "Precision_score: 64.8 \n",
      "recall_score: 68.4 \n",
      "f1_score: 65.3 \n",
      "Epoch 36. Train Loss: 0.16738656079260225 \n",
      " Accuracy of network on 1728 events: 68.4%\n",
      "Accuracy of network on 681 abevents: 33.6%\n",
      "Accuracy on 1047 normal events: 91.0%\n",
      "Accuracy on 234 hypopnea events: 17.5%\n",
      "Accuracy on 447 apnea events: 42.1% \n",
      "\n",
      "Precision_score: 65.1 \n",
      "recall_score: 68.2 \n",
      "f1_score: 65.4 \n",
      "Epoch 37. Train Loss: 0.166329830058324 \n",
      " Accuracy of network on 1728 events: 68.2%\n",
      "Accuracy of network on 681 abevents: 34.1%\n",
      "Accuracy on 1047 normal events: 90.4%\n",
      "Accuracy on 234 hypopnea events: 20.1%\n",
      "Accuracy on 447 apnea events: 41.4% \n",
      "\n",
      "Precision_score: 65.6 \n",
      "recall_score: 68.9 \n",
      "f1_score: 66.0 \n",
      "Epoch 38. Train Loss: 0.1641675577873212 \n",
      " Accuracy of network on 1728 events: 68.9%\n",
      "Accuracy of network on 681 abevents: 35.8%\n",
      "Accuracy on 1047 normal events: 90.4%\n",
      "Accuracy on 234 hypopnea events: 17.9%\n",
      "Accuracy on 447 apnea events: 45.2% \n",
      "\n",
      "Precision_score: 65.7 \n",
      "recall_score: 68.9 \n",
      "f1_score: 66.1 \n",
      "Epoch 39. Train Loss: 0.16529332525190452 \n",
      " Accuracy of network on 1728 events: 68.9%\n",
      "Accuracy of network on 681 abevents: 35.7%\n",
      "Accuracy on 1047 normal events: 90.5%\n",
      "Accuracy on 234 hypopnea events: 19.7%\n",
      "Accuracy on 447 apnea events: 44.1% \n",
      "\n",
      "Precision_score: 66.2 \n",
      "recall_score: 69.1 \n",
      "f1_score: 66.6 \n",
      "Epoch 40. Train Loss: 0.16486084657601252 \n",
      " Accuracy of network on 1728 events: 69.1%\n",
      "Accuracy of network on 681 abevents: 37.0%\n",
      "Accuracy on 1047 normal events: 90.0%\n",
      "Accuracy on 234 hypopnea events: 22.6%\n",
      "Accuracy on 447 apnea events: 44.5% \n",
      "\n",
      "Precision_score: 65.2 \n",
      "recall_score: 68.6 \n",
      "f1_score: 65.7 \n",
      "Epoch 41. Train Loss: 0.16553347922637818 \n",
      " Accuracy of network on 1728 events: 68.6%\n",
      "Accuracy of network on 681 abevents: 34.7%\n",
      "Accuracy on 1047 normal events: 90.6%\n",
      "Accuracy on 234 hypopnea events: 18.4%\n",
      "Accuracy on 447 apnea events: 43.2% \n",
      "\n",
      "Precision_score: 65.8 \n",
      "recall_score: 69.3 \n",
      "f1_score: 66.2 \n",
      "Epoch 42. Train Loss: 0.164600865818795 \n",
      " Accuracy of network on 1728 events: 69.3%\n",
      "Accuracy of network on 681 abevents: 35.8%\n",
      "Accuracy on 1047 normal events: 91.0%\n",
      "Accuracy on 234 hypopnea events: 17.5%\n",
      "Accuracy on 447 apnea events: 45.4% \n",
      "\n",
      "Precision_score: 65.7 \n",
      "recall_score: 68.4 \n",
      "f1_score: 65.4 \n",
      "Epoch 43. Train Loss: 0.16262684090464724 \n",
      " Accuracy of network on 1728 events: 68.4%\n",
      "Accuracy of network on 681 abevents: 33.0%\n",
      "Accuracy on 1047 normal events: 91.4%\n",
      "Accuracy on 234 hypopnea events: 21.8%\n",
      "Accuracy on 447 apnea events: 38.9% \n",
      "\n",
      "Precision_score: 66.0 \n",
      "recall_score: 68.6 \n",
      "f1_score: 65.9 \n",
      "Epoch 44. Train Loss: 0.164225527254338 \n",
      " Accuracy of network on 1728 events: 68.6%\n",
      "Accuracy of network on 681 abevents: 34.2%\n",
      "Accuracy on 1047 normal events: 91.0%\n",
      "Accuracy on 234 hypopnea events: 23.5%\n",
      "Accuracy on 447 apnea events: 39.8% \n",
      "\n",
      "Precision_score: 66.5 \n",
      "recall_score: 69.2 \n",
      "f1_score: 66.4 \n",
      "Epoch 45. Train Loss: 0.16522731331677967 \n",
      " Accuracy of network on 1728 events: 69.2%\n",
      "Accuracy of network on 681 abevents: 35.2%\n",
      "Accuracy on 1047 normal events: 91.3%\n",
      "Accuracy on 234 hypopnea events: 21.8%\n",
      "Accuracy on 447 apnea events: 42.3% \n",
      "\n",
      "Precision_score: 65.1 \n",
      "recall_score: 67.8 \n",
      "f1_score: 64.8 \n",
      "Epoch 46. Train Loss: 0.16346682404526472 \n",
      " Accuracy of network on 1728 events: 67.8%\n",
      "Accuracy of network on 681 abevents: 31.7%\n",
      "Accuracy on 1047 normal events: 91.3%\n",
      "Accuracy on 234 hypopnea events: 21.8%\n",
      "Accuracy on 447 apnea events: 36.9% \n",
      "\n",
      "Precision_score: 66.3 \n",
      "recall_score: 69.0 \n",
      "f1_score: 66.8 \n",
      "Epoch 47. Train Loss: 0.16321149099313997 \n",
      " Accuracy of network on 1728 events: 69.0%\n",
      "Accuracy of network on 681 abevents: 37.4%\n",
      "Accuracy on 1047 normal events: 89.5%\n",
      "Accuracy on 234 hypopnea events: 23.5%\n",
      "Accuracy on 447 apnea events: 44.7% \n",
      "\n",
      "Precision_score: 65.5 \n",
      "recall_score: 68.4 \n",
      "f1_score: 65.6 \n",
      "Epoch 48. Train Loss: 0.16359006641786308 \n",
      " Accuracy of network on 1728 events: 68.4%\n",
      "Accuracy of network on 681 abevents: 34.1%\n",
      "Accuracy on 1047 normal events: 90.7%\n",
      "Accuracy on 234 hypopnea events: 20.9%\n",
      "Accuracy on 447 apnea events: 40.9% \n",
      "\n",
      "Precision_score: 65.2 \n",
      "recall_score: 68.8 \n",
      "f1_score: 65.7 \n",
      "Epoch 49. Train Loss: 0.16112130768292002 \n",
      " Accuracy of network on 1728 events: 68.8%\n",
      "Accuracy of network on 681 abevents: 34.7%\n",
      "Accuracy on 1047 normal events: 90.9%\n",
      "Accuracy on 234 hypopnea events: 17.5%\n",
      "Accuracy on 447 apnea events: 43.6% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a model and train\n",
    "\n",
    "# some arguments for training: loss, epoch, accuracy\n",
    "model = re_model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = getattr(torch.optim, optim)(model.parameters(), lr=lr)\n",
    "classes = ('normal', 'hypopnea', 'apnea')\n",
    "class_total = [0, 0, 0]\n",
    "class_correct = [0, 0, 0]\n",
    "train_loss = 0\n",
    "best_loss = 100\n",
    "best_acc = []\n",
    "epochs=50\n",
    "\n",
    "for ep in range(epochs):\n",
    "    # record prediction results\n",
    "    class_total = list(0. for i in range(3))\n",
    "    class_correct = list(0. for i in range(3))\n",
    "    train_loss = 0\n",
    "    best_loss = 100\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        if data.shape[0] == 1:\n",
    "            continue\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    numpy_Y = np.array([])\n",
    "    numpy_pred = np.array([])\n",
    "    for seqs, labels in val_loader:\n",
    "        output = model(seqs)\n",
    "        _,prediction = torch.max(output.data, 1)\n",
    "        pre = (labels == prediction.data)\n",
    "        numpy_Y = np.concatenate((numpy_Y, labels.numpy()))\n",
    "        numpy_pred = np.concatenate((numpy_pred, prediction.numpy()))\n",
    "        for y_i in range(len(labels)):\n",
    "            label = labels[y_i]\n",
    "            class_total[label] += 1\n",
    "            class_correct[label] += pre[y_i].item()\n",
    "            \n",
    "    score(numpy_Y, numpy_pred)\n",
    "    adjust_learning_rate(lr, optimizer, ep)\n",
    "    accs = []\n",
    "    accs.append(\"Accuracy of network on %d events: %.1f%%\" %(sum(class_total),  sum(class_correct) / sum(class_total) *100))\n",
    "    accs.append(\"Accuracy of network on %d abevents: %.1f%%\" %(class_total[1]+class_total[2], (class_correct[1]+class_correct[2])/(class_total[1]+class_total[2]) *100))\n",
    "    accs.extend([\"Accuracy on %d %s events: %.1f%%\" %(class_total[i], classes[i], class_correct[i] / class_total[i] * 100) for i in range(3)])\n",
    "\n",
    "    train_loss = train_loss/(len(train_set))\n",
    "    # save the best model\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        best_acc = accs\n",
    "        torch.save(model.state_dict(), \"./checkpoints/cnn_models/{}_params.pkl\".format(str(model)[:3]))\n",
    "        #log_model(model, optimizer, criterion, sum(class_total), best_loss, accs)\n",
    "    epoch_str = (\"Epoch {}. Train Loss: {}\".format(ep, train_loss))\n",
    "    print(epoch_str, \"\\n\",\"\\n\".join(accs), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN预测结果\n",
    "numpy_Y = np.array([])\n",
    "numpy_pred = np.array([])\n",
    "# 预测结果（每个类别的得分）\n",
    "cnn_pred_test_score = np.array([0, 0, 0])\n",
    "cnn_pred_test_score.shape = (1,3)\n",
    "count = 0\n",
    "# validation dataset\n",
    "for seqs, labels in val_loader:\n",
    "    output = model(seqs)\n",
    "    _,prediction = torch.max(output.data, 1)\n",
    "    pre = (labels == prediction.data)\n",
    "    numpy_Y = np.concatenate((numpy_Y, labels.numpy()))\n",
    "    numpy_pred = np.concatenate((numpy_pred, prediction.numpy()))\n",
    "    cnn_pred_test_score = np.concatenate((cnn_pred_test_score, nn.functional.softmax(output.data, dim=1).numpy()), axis=0)\n",
    "    \n",
    "    #print(nn.functional.softmax(output.data, dim=1).numpy())\n",
    "    for y_i in range(len(labels)):\n",
    "        label = labels[y_i]\n",
    "        class_total[label] += 1\n",
    "        class_correct[label] += pre[y_i].item()\n",
    "\n",
    "cnn_pred_test_score = cnn_pred_test_score[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_train_score = np.array([0, 0, 0])\n",
    "cnn_pred_train_score.shape = (1,3)\n",
    "count = 0\n",
    "# train dataset\n",
    "for seqs, labels in train_loader:\n",
    "    output = model(seqs)\n",
    "    _,prediction = torch.max(output.data, 1)\n",
    "    pre = (labels == prediction.data)\n",
    "    cnn_pred_train_score = np.concatenate((cnn_pred_train_score, nn.functional.softmax(output.data, dim=1).numpy()), axis=0)\n",
    "\n",
    "cnn_pred_train_score = cnn_pred_train_score[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.logger import SimpleLogger\n",
    "\n",
    "init_message = \"xavier_uniform\"\n",
    "\n",
    "total = len(train_set)\n",
    "logger = SimpleLogger(log_path='./log/', log_name='pulse_classifier')\n",
    "\n",
    "    # Log\n",
    "ctime = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "logs = {\n",
    "    'Logs': '',\n",
    "    \"Time\": ctime,\n",
    "    # Dataset\n",
    "    'Data size': total,\n",
    "    # Net\n",
    "    'Criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'Net': model,\n",
    "    # Train\n",
    "    'Epochs': 10,\n",
    "    'loss': best_loss,\n",
    "    # Acurracy\n",
    "    'Acurracy': '\\n'.join(accs),\n",
    "    'Init': init_message\n",
    "}\n",
    "\n",
    "logger.log_info(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "何文东\n",
      "杨梓恒\n",
      "林颖晞\n",
      "舒辰熙\n",
      "陈知浅\n",
      "高子羲\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  1.00000000e+02,   9.80000000e+01,   9.88833333e+01,\n",
       "          1.69722222e-01,   4.11973570e-01,   9.90000000e+01,\n",
       "          4.16625892e-03,   1.01129277e+00,   9.91066914e-01,\n",
       "         -8.16832524e-01,   2.25701386e+00,   0.00000000e+00,\n",
       "          6.77966102e-02,  -1.00000000e+00,   0.00000000e+00,\n",
       "          1.31705989e+01,   2.00000000e+00,   1.00000000e+00,\n",
       "          3.00000000e+00,   9.88833333e+01,   1.72598870e-01,\n",
       "          1.00000000e+02,   9.80000000e+01,  -8.16832524e-01,\n",
       "          2.25701386e+00]), 4029, 1728, [3360.0, 985.0, 1412.0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = BODataSet(root=root)\n",
    "class_labels = ('normal', 'hypopnea', 'apnea')\n",
    "X, Y = dataset.get_data()\n",
    "# train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3, random_state=33)\n",
    "train_X, test_X, train_Y, test_Y = X[:int(len(X)*0.7)], X[int(len(X)*0.7):], Y[:int(len(X)*0.7)], Y[int(len(X)*0.7):]\n",
    "train_X[0], len(train_X), len(test_X), dataset.class_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code \n",
    "def test(clf, X, Y):\n",
    "#     test models for specify dataset\n",
    "    class_labels = ('normal', 'hypopnea', 'apnea')\n",
    "    class_total = [0, 0, 0]\n",
    "    class_correct = [0, 0, 0]\n",
    "    pre_Y = clf.predict(X)\n",
    "    prediction = (pre_Y == Y)\n",
    "    for index in range(len(Y)):\n",
    "        label = Y[index]\n",
    "        class_total[label] += 1\n",
    "        class_correct[label] += prediction[index]\n",
    "    \n",
    "    accs = []\n",
    "    accs.append(\"Total %d events accuracy: %.1f %%\"%(sum(class_total), sum(class_correct) / sum(class_total) *100))\n",
    "    accs.append(\"Total %d abnormal accuracy: %.1f %%\" %(class_total[1]+class_total[2], (class_correct[1]+class_correct[2])/(class_total[1]+class_total[2]) *100))\n",
    "    accs.extend([\"%s %d events accuracy: %.1f %%\" %(class_labels[i], class_total[i], class_correct[i] / class_total[i] *100 if class_total[i] != 0 else -1) for i in range(3)])\n",
    "    print('\\n'.join(accs), \"\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"Precision_score: %.1f \" % (metrics.precision_score(Y, pre_Y, average='weighted')*100))\n",
    "    print(\"recall_score: %.1f \" % (metrics.recall_score(Y, pre_Y, average='weighted')*100))\n",
    "    print(\"f1_score: %.1f \" % (metrics.f1_score(Y, pre_Y, average='weighted')*100))\n",
    "    return pre_Y\n",
    "\n",
    "# compute accuracy\n",
    "def calc_accuracy(Y, pre_Y):\n",
    "    class_labels = ('normal', 'hypopnea', 'apnea')\n",
    "    class_total = [0, 0, 0]\n",
    "    class_correct = [0, 0, 0]\n",
    "    prediction = (pre_Y == Y)\n",
    "    #print(prediction)\n",
    "    for index in range(len(Y)):\n",
    "        label = Y[index]\n",
    "        class_total[label] += 1\n",
    "        class_correct[label] += prediction[index]\n",
    "    \n",
    "    accs = []\n",
    "    accs.append(\"Total %d events accuracy: %.1f %%\"%(sum(class_total), sum(class_correct) / sum(class_total) *100))\n",
    "    accs.append(\"Total %d abnormal accuracy: %.1f %%\" %(class_total[1]+class_total[2], (class_correct[1]+class_correct[2])/(class_total[1]+class_total[2]) *100))\n",
    "    accs.extend([\"%s %d events accuracy: %.1f %%\" %(class_labels[i], class_total[i], class_correct[i] / class_total[i] *100 if class_total[i] != 0 else -1) for i in range(3)])\n",
    "    \n",
    "    print('\\n'.join(accs), \"\\n\")\n",
    "    \n",
    "    print(\"Precision_score: %.1f \" % (metrics.precision_score(Y, pre_Y, average='weighted')*100))\n",
    "    print(\"recall_score: %.1f \" % (metrics.recall_score(Y, pre_Y, average='weighted')*100))\n",
    "    print(\"f1_score: %.1f \" % (metrics.f1_score(Y, pre_Y, average='weighted')*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10426\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 35 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=8)]: Done 105 out of 105 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1728 events accuracy: 72.2 %\n",
      "Total 681 abnormal accuracy: 44.9 %\n",
      "normal 1047 events accuracy: 89.9 %\n",
      "hypopnea 234 events accuracy: 30.3 %\n",
      "apnea 447 events accuracy: 52.6 % \n",
      "\n",
      "Precision_score: 70.7 \n",
      "recall_score: 72.2 \n",
      "f1_score: 71.0 \n",
      "100 0.001\n"
     ]
    }
   ],
   "source": [
    "# train a SVM model\n",
    "svm_clf = svm.SVC()\n",
    "svm_clf = svm.SVC(decision_function_shape='ovo', C=0.001, gamma=0.0001)\n",
    "param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma':[0.0001, 0.001, 0.01, 0.1, 0.3],}\n",
    "\n",
    "svm_clf_cv = GridSearchCV(svm.SVC(decision_function_shape='ovo', kernel='rbf'),param_grid, n_jobs=8, verbose=2, scoring='f1_macro')\n",
    "svm_clf_cv.fit(X,Y)\n",
    "\n",
    "svm_clf = svm.SVC(decision_function_shape='ovo', C=svm_clf_cv.best_params_['C'], gamma=svm_clf_cv.best_params_['gamma'], probability=True)\n",
    "svm_clf.fit(train_X, train_Y)\n",
    "\n",
    "test(svm_clf, test_X, test_Y)\n",
    "\n",
    "\n",
    "print(svm_clf_cv.best_params_['C'], svm_clf_cv.best_params_['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1728 events accuracy: 72.2 %\n",
      "Total 681 abnormal accuracy: 44.9 %\n",
      "normal 1047 events accuracy: 89.9 %\n",
      "hypopnea 234 events accuracy: 30.3 %\n",
      "apnea 447 events accuracy: 52.6 % \n",
      "\n",
      "Precision_score: 70.7 \n",
      "recall_score: 72.2 \n",
      "f1_score: 71.0 \n"
     ]
    }
   ],
   "source": [
    "# get svm predict score\n",
    "import pylab as pl\n",
    "svm_clf.fit(train_X,train_Y)\n",
    "test(svm_clf, test_X, test_Y)\n",
    "len(svm_clf.support_vectors_)\n",
    "svm_pred_train_score = svm_clf.predict_proba(train_X)\n",
    "svm_pred_test_score = svm_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pre = svm_clf.predict(test_X)\n",
    "mat = confusion_matrix(test_Y, pre)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "# plt.figure()\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "plt.rcParams['figure.dpi'] = 100 #分辨率\n",
    "plt.savefig(\"./cm_svm\")\n",
    "# plt.rcParams.update({'font.size': 40})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clf.best_estimator_.get_params())\n",
    "rf_clf = RandomForestClassifier(max_depth=20, max_features=0.7, criterion='gini', max_leaf_nodes=500, min_samples_leaf=1, min_samples_split=2, n_estimators=10)\n",
    "rf_clf.fit(train_X, train_Y)\n",
    "test(rf_clf, test_X, test_Y)\n",
    "rf_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softmax',  # 多分类的问题\n",
    "    'num_class': 3,               # 类别数，与 multisoftmax 并用\n",
    "    'gamma': 0.1,                  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "    'max_depth': 12,               # 构建树的深度，越大越容易过拟合\n",
    "    'lambda': 2,                   # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    'subsample': 0.7,              # 随机采样训练样本\n",
    "    'colsample_bytree': 0.7,       # 生成树时进行的列采样\n",
    "    'min_child_weight': 3,\n",
    "    'silent': 1,                   # 设置成1则没有运行信息输出，最好是设置为0.\n",
    "    'eta': 0.1,                  # 如同学习率\n",
    "    'seed': 1000,\n",
    "    'nthread': 4,                  # cpu 线程数\n",
    "}\n",
    "plst = params.items()\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, train_Y)\n",
    "d_test_X = xgb.DMatrix(test_X)\n",
    "# model = xgb.train(plst, dtrain, 100)\n",
    "\n",
    "# test(model, d_test_X, test_Y)\n",
    "\n",
    "xgb_clf = XGBClassifier(learning_rate=0.1)\n",
    "xgb_clf.fit(np.array(train_X), train_Y)\n",
    "\n",
    "test(xgb_clf, np.array(test_X), test_Y)\n",
    "xgb_pred_train_score = xgb_clf.predict_proba(np.array(train_X))\n",
    "xgb_pred_test_score = xgb_clf.predict_proba(np.array(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(np.array(train_X), train_Y)\n",
    "test_data = lgb.Dataset(np.array(test_X), label=test_Y)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'n_estimators': 30,\n",
    "    'objective': 'multiclass',\n",
    "    'max_depth': 50,\n",
    "    'num_class': 3,\n",
    "#     'metric': 'auc',\n",
    "    'learning_rate': 0.1,\n",
    "#     'verbose': 0\n",
    "}\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier()\n",
    "lgb_clf.fit(train_X, train_Y)\n",
    "# lgb_clf = lgb.train(params, train_data, valid_sets=test_data)\n",
    "\n",
    "y_pred = lgb_clf.predict(test_X)\n",
    "# y_pred = np.array([list(e).index(max(e)) for e in y_pred])\n",
    "\n",
    "calc_accuracy(test_Y, y_pred)\n",
    "lgb_pred_train_score = lgb_clf.predict_proba(train_X)\n",
    "lgb_pred_test_score = lgb_clf.predict_proba(test_X)\n",
    "# bst.save_model('model.txt')\n",
    "# json_model = bst.dump_model()\n",
    "# bst = lgb.Booster(model_file='model.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_pred_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_clf = GradientBoostingClassifier(random_state=10)\n",
    "gbdt_clf.fit(train_X, train_Y)\n",
    "\n",
    "test(gbdt_clf, test_X, test_Y)\n",
    "\n",
    "gbdt_pred_train_score = gbdt_clf.predict_proba(train_X)\n",
    "gbdt_pred_test_score = gbdt_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=1, p=5, metric='minkowski')\n",
    "knn_clf.fit(train_X,train_Y)\n",
    "\n",
    "test(knn, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desicion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=10)\n",
    "dt_clf.fit(train_X, train_Y)\n",
    "\n",
    "test(dt_clf, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 传统模型自我融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[('svm', svm_clf), ('lgb', lgb_clf), ('gbdt', gbdt_clf)])\n",
    "eclf = eclf.fit(np.array(train_X),train_Y)\n",
    "pre_Y = test(eclf, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "# test code for fusion models\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pre = eclf.predict(test_X)\n",
    "mat = confusion_matrix(test_Y, pre)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "# plt.figure()\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "plt.rcParams['figure.dpi'] = 100 #分辨率\n",
    "plt.savefig(\"./cm_svm\")\n",
    "# plt.rcParams.update({'font.size': 40})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN和传统模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_test_score = np.concatenate((svm_pred_test_score, cnn_pred_test_score), axis=1)\n",
    "fusion_train_score = np.concatenate((svm_pred_train_score, cnn_pred_train_score), axis=1)\n",
    "fusion_train_score = torch.from_numpy(fusion_train_score).float()\n",
    "fusion_test_score = torch.from_numpy(fusion_test_score).float()\n",
    "fusion_train_set = list(zip(fusion_train_score, train_Y))\n",
    "fusion_test_set = list(zip(fusion_test_score, test_Y))\n",
    "\n",
    "fusion_train_score.shape, fusion_test_score.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fusion_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fusion_network,self).__init__()\n",
    "        self.linear = nn.Linear(6,3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_train_loader = torch.utils.data.DataLoader(fusion_train_set, batch_size=batch_size, shuffle=True)\n",
    "fusion_val_loader = torch.utils.data.DataLoader(fusion_test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_model = fusion_network()\n",
    "\n",
    "fusion_criterion = torch.nn.CrossEntropyLoss()\n",
    "fusion_optimizer = getattr(torch.optim, optim)(model.parameters(), lr=lr)\n",
    "classes = ('normal', 'hypopnea', 'apnea')\n",
    "best_acc = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    class_total = list(0. for i in range(3))\n",
    "    class_correct = list(0. for i in range(3))\n",
    "    train_loss = 0\n",
    "    best_loss = 100\n",
    "    for batch_idx, (data, target) in enumerate(fusion_train_loader):\n",
    "        fusion_optimizer.zero_grad()\n",
    "        if data.shape[0] == 1:\n",
    "            continue\n",
    "        output = fusion_model(data)\n",
    "        loss = fusion_criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    numpy_Y = np.array([])\n",
    "    numpy_pred = np.array([])\n",
    "    for seqs, labels in fusion_val_loader:\n",
    "        output = fusion_model(seqs)\n",
    "        _,prediction = torch.max(output.data, 1)\n",
    "        pre = (labels == prediction.data)\n",
    "        numpy_Y = np.concatenate((numpy_Y, labels.numpy()))\n",
    "        numpy_pred = np.concatenate((numpy_pred, prediction.numpy()))\n",
    "        for y_i in range(len(labels)):\n",
    "            label = labels[y_i]\n",
    "            class_total[label] += 1\n",
    "            class_correct[label] += pre[y_i].item()\n",
    "            \n",
    "    score(numpy_Y, numpy_pred)\n",
    "    adjust_learning_rate(lr, fusion_optimizer, ep)\n",
    "    accs = []\n",
    "    accs.append(\"Accuracy of network on %d events: %.1f%%\" %(sum(class_total),  sum(class_correct) / sum(class_total) *100))\n",
    "    accs.append(\"Accuracy of network on %d abevents: %.1f%%\" %(class_total[1]+class_total[2], (class_correct[1]+class_correct[2])/(class_total[1]+class_total[2]) *100))\n",
    "    accs.extend([\"Accuracy on %d %s events: %.1f%%\" %(class_total[i], classes[i], class_correct[i] / class_total[i] * 100) for i in range(3)])\n",
    "\n",
    "    train_loss = train_loss/(len(train_set))\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        best_acc = accs\n",
    "        torch.save(fusion_model.state_dict(), \"./checkpoints/cnn_models/{}_params.pkl\".format(str(fusion_model)[:3]))\n",
    "        #log_model(model, optimizer, criterion, sum(class_total), best_loss, accs)\n",
    "    epoch_str = (\"Epoch {}. Train Loss: {}\".format(ep, train_loss))\n",
    "    print(epoch_str, \"\\n\",\"\\n\".join(accs), \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
